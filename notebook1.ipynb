{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio comparativo de algoritmos en un problema de bandido de k-brazos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "from src_algorithms import Algorithm, EpsilonGreedy\n",
    "from src_arms import ArmNormal, Bandit\n",
    "from src_plotting import plot_average_rewards, plot_optimal_selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(bandit: Bandit, algorithms: List[Algorithm], steps: int, runs: int):\n",
    "\n",
    "    optimal_arm = bandit.optimal_arm  # Necesario para calcular el porcentaje de selecciones óptimas.\n",
    "\n",
    "    rewards = np.zeros((len(algorithms), steps)) # Matriz para almacenar las recompensas promedio.\n",
    "\n",
    "    optimal_selections = np.zeros((len(algorithms), steps))  # Matriz para almacenar el porcentaje de selecciones óptimas.\n",
    "\n",
    "    np.random.seed(seed)  # Asegurar reproducibilidad de resultados.\n",
    "\n",
    "    for run in range(runs):\n",
    "        current_bandit = Bandit(arms=bandit.arms)\n",
    "\n",
    "        for algo in algorithms:\n",
    "            algo.reset() # Reiniciar los valores de los algoritmos.\n",
    "\n",
    "        total_rewards_per_algo = np.zeros(len(algorithms)) # Acumulador de recompensas por algoritmo. Necesario para calcular el promedio.\n",
    "\n",
    "        for step in range(steps):\n",
    "            for idx, algo in enumerate(algorithms):\n",
    "                chosen_arm = algo.select_arm() # Seleccionar un brazo según la política del algoritmo.\n",
    "                reward = current_bandit.pull_arm(chosen_arm) # Obtener la recompensa del brazo seleccionado.\n",
    "                algo.update(chosen_arm, reward) # Actualizar el valor estimado del brazo seleccionado.\n",
    "\n",
    "                rewards[idx, step] += reward # Acumular la recompensa obtenida en la matriz rewards para el algoritmo idx en el paso step.\n",
    "                total_rewards_per_algo[idx] += reward # Acumular la recompensa obtenida en total_rewards_per_algo para el algoritmo idx.\n",
    "\n",
    "                #TODO: modificar optimal_selections cuando el brazo elegido se corresponda con el brazo óptimo optimal_arm\n",
    "\n",
    "\n",
    "    rewards /= runs\n",
    "\n",
    "    # TODO: calcular el porcentaje de selecciones óptimas y almacenar en optimal_selections\n",
    "\n",
    "    return rewards, optimal_selections\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
