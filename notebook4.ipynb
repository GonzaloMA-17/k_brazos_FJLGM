{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GonzaloMA-17/k_brazos_FJLGM/blob/main/notebook1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cikMoIxi2Sh2"
      },
      "source": [
        "# Estudio comparativo de algoritmos en un problema de bandido de k-brazos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resolución del Problema con Algoritmos Softmax\n",
        "\n",
        "En este estudio, abordaremos el problema del bandido de k-brazos utilizando algoritmos Softmax. Este enfoque es una alternativa a los algoritmos epsilon-greedy y se basa en la probabilidad de seleccionar cada brazo en función de su valor estimado.\n",
        "\n",
        "#### Algoritmos Softmax\n",
        "\n",
        "A diferencia de los algoritmos epsilon-greedy, que seleccionan el mejor brazo conocido con una alta probabilidad y exploran otros brazos con una pequeña probabilidad, los algoritmos Softmax asignan una probabilidad a cada brazo basada en su valor estimado. Esto permite una exploración más equilibrada y puede mejorar el rendimiento en ciertos escenarios.\n",
        "\n",
        "#### Objetivos del Estudio\n",
        "\n",
        "En este estudio, compararemos el rendimiento de diferentes configuraciones del algoritmo Softmax en un entorno de bandido de 10 brazos. Para ello, realizaremos múltiples ejecuciones del experimento y generaremos las siguientes gráficas:\n",
        "\n",
        "1. **Porcentaje de Selección del Brazo**: Mostraremos cómo varía el porcentaje de veces que cada brazo es seleccionado a lo largo del tiempo.\n",
        "2. **Estadísticas de Cada Brazo**: Presentaremos el promedio de las ganancias obtenidas por cada brazo, lo que nos permitirá evaluar la efectividad de las selecciones.\n",
        "3. **Evolución del Rechazo**: Analizaremos cómo evoluciona el rechazo acumulado (diferencia entre la recompensa óptima y la recompensa obtenida) a lo largo del tiempo.\n",
        "\n",
        "Estas visualizaciones nos permitirán entender mejor el comportamiento de los algoritmos Softmax y su capacidad para equilibrar la exploración y la explotación en el problema del bandido de k-brazos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxj2Pzsb2yFO"
      },
      "source": [
        "## Preparación del entorno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxtOOqcn3PCw"
      },
      "source": [
        "**Clonación del repositorio:**\n",
        "\n",
        "Para poder acceder a todas las clases y métodos necesarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZi6fXrC24ls",
        "outputId": "f5e4f26a-9e74-44d6-8d08-62dbaea4da08"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/GonzaloMA-17/k_brazos_FJLGM.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCzfsNXI3LCy",
        "outputId": "f4ef63d8-ec3a-415a-e6ac-d82dbb1f0c40"
      },
      "outputs": [],
      "source": [
        "%cd k_brazos_FJLGM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtmM1A6i3knR"
      },
      "source": [
        "### Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcdEbXJ32Sh4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "from src_algorithms import Algorithm, Softmax, GradientPreference\n",
        "from src_arms import *\n",
        "from src_plotting import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJURYx8s3vaa"
      },
      "source": [
        "Semilla:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fWBjkO5W2Sh5"
      },
      "outputs": [],
      "source": [
        "seed = 1234"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMYZZVtH369b"
      },
      "source": [
        "**Definición del experimento:**\n",
        "\n",
        "Cada algoritmo se ejecuta en un problema de k-armed bandit durante un número de pasos de tiempo y ejecuciones determinado. Se comparan los resultados de los algoritmos en términos de recompensa promedio.\n",
        "\n",
        "Por ejemplo. Dado un bandido de k-brazos, se ejecutan dos algoritmos Softmax con diferentes valores de temperatura. Se estudia la evolución de cada política en un número de pasos, por ejemplo, mil pasos. Entonces se repite el experimento un número de veces, por ejemplo, 500 veces. Es decir, se ejecutan 500 veces la evolución de cada algoritmo en 1000 pasos. Para cada paso calculamos el promedio de las recompensas obtenidas en esas 500 veces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment_v2(bandit: Bandit, algorithms: List[Algorithm], steps: int, runs: int):\n",
        "    optimal_arm = bandit.optimal_arm\n",
        "    optimal_reward = bandit.arms[optimal_arm].mu  # Obtener la recompensa esperada del brazo óptimo\n",
        "\n",
        "    rewards = np.zeros((len(algorithms), steps))  # Matriz para almacenar las recompensas promedio.\n",
        "    optimal_selections = np.zeros((len(algorithms), steps))  # Matriz para almacenar el porcentaje de selecciones óptimas.\n",
        "    arm_rewards = np.zeros((len(algorithms), bandit.k))  # Matriz para almacenar las recompensas acumuladas por brazo.\n",
        "    arm_counts = np.zeros((len(algorithms), bandit.k))  # Matriz para almacenar el número de selecciones por brazo.\n",
        "    regret_accumulated = np.zeros((len(algorithms), steps))  # Matriz para almacenar el rechazo acumulado.\n",
        "\n",
        "    np.random.seed(seed)  # Asegurar reproducibilidad de resultados.\n",
        "\n",
        "    for run in range(runs):\n",
        "        current_bandit = Bandit(arms=bandit.arms)\n",
        "\n",
        "        for algo in algorithms:\n",
        "            algo.reset()  # Reiniciar los valores de los algoritmos.\n",
        "\n",
        "        for step in range(steps):\n",
        "            for idx, algo in enumerate(algorithms):\n",
        "                chosen_arm = algo.select_arm()  # Seleccionar un brazo según la política del algoritmo.\n",
        "                reward = current_bandit.pull_arm(chosen_arm)  # Obtener la recompensa del brazo seleccionado.\n",
        "                algo.update(chosen_arm, reward)  # Actualizar el valor estimado del brazo seleccionado.\n",
        "\n",
        "                rewards[idx, step] += reward  # Acumular la recompensa obtenida en la matriz rewards para el algoritmo idx en el paso step.\n",
        "                arm_rewards[idx, chosen_arm] += reward  # Acumular la recompensa obtenida en arm_rewards para el brazo chosen_arm.\n",
        "                arm_counts[idx, chosen_arm] += 1  # Incrementar el conteo de selecciones para el brazo chosen_arm.\n",
        "\n",
        "                # Modificar optimal_selections cuando el brazo elegido se corresponda con el brazo óptimo optimal_arm\n",
        "                if chosen_arm == optimal_arm:\n",
        "                    optimal_selections[idx, step] += 1\n",
        "\n",
        "                # Calcular el rechazo acumulado\n",
        "                regret_accumulated[idx, step] += optimal_reward - reward\n",
        "\n",
        "    rewards /= runs\n",
        "\n",
        "    # Calcular el porcentaje de selecciones óptimas y almacenar en optimal_selections\n",
        "    optimal_selections = (optimal_selections / runs) * 100\n",
        "\n",
        "    # Calcular el promedio de ganancias de cada brazo\n",
        "    average_rewards = arm_rewards / np.maximum(arm_counts, 1)\n",
        "\n",
        "    # Calcular el rechazo acumulado promedio\n",
        "    regret_accumulated = np.cumsum(regret_accumulated, axis=1) / runs\n",
        "\n",
        "    # Preparar las estadísticas de los brazos\n",
        "    arm_stats = [{'average_rewards': average_rewards[idx], 'selection_counts': arm_counts[idx]} for idx in range(len(algorithms))]\n",
        "\n",
        "    return rewards, optimal_selections, arm_stats, regret_accumulated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrmNLDz6TYCL"
      },
      "source": [
        "**Ejecución del experimento**:\n",
        "\n",
        "Se realiza el experimento usando 10 brazos, cada uno de acuerdo a una distribución Bernoulli de probabilidad **p**. \n",
        "\n",
        "Se realizan 500 ejecuciones de 1000 pasos cada una. Se contrastan dos algoritmos de la familia de **Ascenso del gradiente**: \n",
        "\n",
        "- *Softmax*\n",
        "- *Gradiente de preferencias*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuNq47oZTYCL",
        "outputId": "55ada15a-b8cc-42a6-b11b-4c6851211f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bandit with 10 arms: ArmBernoulli(p=0.25), ArmBernoulli(p=0.6), ArmBernoulli(p=0.45), ArmBernoulli(p=0.72), ArmBernoulli(p=0.73), ArmBernoulli(p=0.32), ArmBernoulli(p=0.74), ArmBernoulli(p=0.87), ArmBernoulli(p=0.8), ArmBernoulli(p=0.39)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(seed)  # Fijar la semilla para reproducibilidad\n",
        "\n",
        "k = 10  # Número de brazos\n",
        "steps = 1000  # Número de pasos que se ejecutarán cada algoritmo\n",
        "runs = 500  # Número de ejecuciones\n",
        "\n",
        "# Creación del bandit\n",
        "bandit = Bandit(arms=ArmBernoulli.generate_arms(k)) # Generar un bandido con k brazos de distribución normal\n",
        "print(bandit)\n",
        "\n",
        "optimal_arm = bandit.optimal_arm\n",
        "print(f\"Optimal arm: {optimal_arm + 1} with expected reward={bandit.get_expected_value(optimal_arm)}\")\n",
        "\n",
        "# Definir los algoritmos a comparar. En este caso son 3 algoritmos epsilon-greedy con diferentes valores de epsilon.\n",
        "algorithms = [Softmax(k=k, tau=1),Softmax(k=k, tau=2.5), GradientPreference(k=k, alpha=0.1), GradientPreference(k=k, alpha=0.4)]\n",
        "\n",
        "# Ejecutar el experimento y obtener las recompensas promedio y promedio de las selecciones óptimas\n",
        "rewards, optimal_selections, armStats, regret = run_experiment_v2(bandit, algorithms, steps, runs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8P4QROYTYCM"
      },
      "source": [
        "**Visualización de resultados**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "oDy_plG3TYCM",
        "outputId": "a6bc96fd-8119-4d98-81de-10c4a1d16c30"
      },
      "outputs": [],
      "source": [
        "# Graficar los resultados\n",
        "plot_average_rewards(steps, rewards, algorithms)\n",
        "# plot_optimal_selections(steps, optimal_selections, algorithms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "Z8LYVpyoTYCM",
        "outputId": "fa848eba-ec67-478a-a197-e0b3a270b652"
      },
      "outputs": [],
      "source": [
        "plot_optimal_selections(steps, optimal_selections, algorithms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tY-v6-q7TYCM",
        "outputId": "24f9dcda-1271-49f1-8eab-ee5c83132a44"
      },
      "outputs": [],
      "source": [
        "plot_arm_statistics(armStats, algorithms, optimal_arm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_regret(steps, regret, algorithms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LohSR0rATYCM"
      },
      "source": [
        "**Análisis detallado de la imagen**\n",
        "\n",
        "La imagen muestra un gráfico de líneas titulado \"Recompensa Promedio vs Pasos de Tiempo\", donde se analiza el desempeño de diferentes estrategias del algoritmo ε-Greedy en un entorno de multi-armed bandit. En el eje x se representan los pasos de tiempo, mientras que en el eje y se muestra la recompensa promedio obtenida por cada algoritmo.\n",
        "\n",
        "Tres líneas de colores distintos representan diferentes valores de ε en el algoritmo ε-Greedy:\n",
        "\n",
        "- Azul (ε = 0): Representa una estrategia completamente explotadora, es decir, que siempre elige la acción que ha dado la mejor recompensa hasta ahora sin explorar nuevas opciones.\n",
        "- Naranja (ε = 0.01): Representa una estrategia con una pequeña probabilidad del 1% de elegir una acción aleatoria (exploración).\n",
        "- Verde (ε = 0.1): Representa una estrategia con un 10% de probabilidad de explorar acciones aleatorias.\n",
        "\n",
        "**Crecimiento de la recompensa promedio:**\n",
        "\n",
        "La línea verde (ε=0.1) alcanza rápidamente una recompensa promedio alta, lo que indica que la estrategia con mayor exploración aprende más rápido qué brazos del bandit son óptimos.\n",
        "La línea naranja (ε=0.01) también muestra un crecimiento, pero más lento en comparación con ε=0.1.\n",
        "La línea azul (ε=0) se mantiene en un nivel bajo de recompensa, lo que sugiere que no logra encontrar el mejor brazo porque no explora nuevas opciones.\n",
        "\n",
        "**Conclusiones**\n",
        "\n",
        "Hemos estudiado un experimento de toma de decisiones secuenciales, modelado con un Multi-Armed Bandit (MAB). Este problema es fundamental en el aprendizaje por refuerzo y la teoría de decisiones. La idea principal es que un agente debe aprender cuál es la mejor acción (brazo del bandit) a partir de la experiencia acumulada. Para este estudio nos hemos centrado solo en el estudio del algoritmo epsilon-greedy, llegando a las siguientes conclusiones a partir de los resultados obtenidos y la gráfica generada:\n",
        "\n",
        "1. Exploración vs Explotación\n",
        "\n",
        "El algoritmo ε-Greedy equilibra la exploración y la explotación:\n",
        "\n",
        "Explotación (ε=0): Siempre elige la mejor opción conocida, pero si inicialmente se selecciona un brazo subóptimo, nunca descubrirá otras opciones más rentables.\n",
        "Exploración (ε>0): Introduce aleatoriedad en la selección de acciones para descubrir nuevas opciones potencialmente mejores.\n",
        "El gráfico confirma este comportamiento:\n",
        "\n",
        "ε=0.1 (verde) obtiene la mejor recompensa promedio a lo largo del tiempo porque explora lo suficiente como para encontrar rápidamente el mejor brazo.\n",
        "ε=0.01 (naranja) explora menos, por lo que tarda más en converger a una recompensa alta.\n",
        "ε=0 (azul) no explora en absoluto y queda atrapado en una recompensa subóptima.\n",
        "\n",
        "2. Convergencia de los algoritmos\n",
        "\n",
        "Los algoritmos con mayor exploración (ε=0.1) alcanzan una recompensa alta más rápido. Esto se debe a que:\n",
        "\n",
        "Al principio, el algoritmo no tiene información suficiente sobre cuál es el mejor brazo.\n",
        "Con el tiempo, al realizar exploraciones, descubre cuál es el mejor brazo y empieza a explotarlo más.\n",
        "Un balance entre exploración y explotación es clave para maximizar la recompensa a largo plazo.\n",
        "\n",
        "3. Aplicaciones y conclusiones\n",
        "\n",
        "En problemas de toma de decisiones (ejemplo: recomendaciones, optimización de anuncios, medicina personalizada), una estrategia de exploración moderada como ε=0.1 es más efectiva para encontrar la mejor opción rápidamente.\n",
        "La falta de exploración (ε=0) lleva a un desempeño deficiente, ya que el agente puede quedarse atrapado en una elección subóptima.\n",
        "En conclusión, el gráfico muestra cómo un nivel adecuado de exploración mejora significativamente el rendimiento del algoritmo en un entorno de aprendizaje por refuerzo. 🚀"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
